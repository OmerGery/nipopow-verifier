\chapter{Implementation}

\todo{write intro}

% In this chapter, we put forth our research and implementation. First, we
% analyze previous work regarding gas consumption, cost in fiat and security.
% Then we gradually describe our improved methodology that eliminates storage by
% utilizing hash-and-resubmit technique and delivers a gas-efficient, superlight
% Bitcoin client implementation.

\section{Analysis of Previous Work}

In this section, we analyse previous work by Christoglou et. al. First we
discuss the process needed to prepare the code for our analysis. Then, we show
the gas usage of all internal functions of the verifier, and the cost of using
this implementation. Finally, we present the vulnerabilities we discovered, and
how we mitigated them in our work.

\subsection{Porting from old Solidity version}

We used the latest version of Solidity compiler for our analysis. In order to
perform this analysis, we needed to port the verifier from version Solidity 0.4
to version 0.6. The changes we needed to perform were mostly syntactic. These
includes the usage of \texttt{abi.encodePacked}, explicit \texttt{memory} and
\texttt{storage} declaration and explicit cast from \texttt{address} to
\texttt{payable address}. We also used our configured EVMs with EIP
2028~\cite{EIP2028} enabled to benefit from low cost function calls. The
functionality of the contract remained equivalent.

\todo{
The proverâ€™s output is a Proof of Proof of Work that satisfies the above
security parameters. The prover needed to be enhanced in order to create
special test cases (Section(ref)) and enable our optimized architecture
(Section(ref)).
}

\subsection{Gas analysis}

Our profiler measures gas usage per line of code. This is very helpful to
observe detailed gas consumption of a contract. Also, we used Solidity events
to measure aggregated gas consumption of different high-level functionalities
by utilizing the build-in \texttt{gasleft()} function. For our experiment, we
used a chain of 75 blocks and a forked chain at index 55 that spans 10
additional blocks as displayed in Figure \ref{figure:proofs_65-10+20}. Detailed
gas usage of the functionalities of the verifier is shown in Table
\ref{table:old_gas_usage}.

\input{figures/proofs_65-10+20.tex}

In this scenario, the original proof is created by an adversary for an event
that does not exist in the honest chain. The proof is contested by an honest
party. We select this configuration because it includes both phases
(submit and contest) and provides full code coverage of each phase since all
underlying operations are executed and no early returns occur.

For a chain of 75 blocks, each phase of the contract needed more than 10
million gas units. Although the size of this test chain is only a very small
faction of the size of a realistic chain, the gas usage already exceeds the
limit of Ethereum blockchain, which is slightly below 10 million. In
particular, the submit of a 650,0000-blocks chain demands 47,280,453 gas. In
Figure \ref{figure:old_gas_per_chain_proof}, we show gas consumption of the submit
phase for different chain sizes and their corresponding proofs sizes. We
demonstrate results for chain sizes from 100 blocks (corresponding to proof size
25) to 650,000 blocks (corresponding to proof size 250).

The linear relation displayed in Figure \ref{figure:old_gas_per_proof} implies
that the gas consumption of the verifier is determined by the size of the
proofs. As shown in Figure \ref{figure:size_of_nipopows} \todo{insert figure
for proof size with relation to the chain in Background}, the size of the
proofs grows logarithmically to the size of the chain, and this is also
reflected to the gas consumption curve.

\input{figures/old_gas_per_proof}

\input{tables/old_cost_in_fiat.tex}

\paragraph{Pricing}

So far, we have shown that the verifier is not applicable to the real
blockchain due to extensive gas usage, exceeding the build-in limitation the
Ethereum blockchain by far. While Ethereum adapts to community demands and
build-in parameters can change, it seem improbable to ever incorporate such a
high block gas limit. However, even in this extreme case, the verifier would
still be impractical due to the high cost of the operations in fiat. We call
this amount \emph{toll}, because it is the cost of using the ``bridge'' between
two blockchains. We list these tolls in Table \ref{table:old_cost_in_fiat}. For
this price, we used gas price equal to 5 Gwei, which is considered a
particularly low amount to complete transactions. With this gas price, the
probability approximately that the transaction will be included in one of the
next 200 blocks is 33\%. Note that low and average gas price will not be
sufficient for contesting phase, and it has to be performed with higher gas
price because of the limited contesting period. We will later analyze
thoroughly the entire spectrum of gas prices and tolls for realistic usage of
both submit and contest phases.

\subsection{Security Analysis}

We observed that previous work is vulnerable to \emph{premining attack}. Here,
we lay out the attack and show how our work mitigates the threat.
\subsubsection{Premining} Premining is the creation of a number of
cryptocurrency coins before the cryptocurrency is launched to the public. Many
altcoins are based on premining~\cite{premine}. Bitcoin, however, is \emph{not}
a premined cryptocurrency. It is proven that every Bitcoin has been created
after 3/Jan/2009, as we describe in Section \ref{premine}. In a blockchain
where such a guarantee did not exist, the creator of the chain could quietly
mine blocks for a long time before initiating the public network as displayed
in ~\ref{figure:premining_attack}. The adversary could then publish the
private, longer chain, invalidating the public chain which is adopted by the
honest users and hosts all their funds.

\input{figures/premining_attack}

The NIPoPoW protocol takes into consideration the $genesis$ block of the
underlying blockchain. We remind that the first block of the chain is always
included in the NIPoPoW by construction. As we discuss in
Section~\ref{NIPoPoWs}, in the NIPoPoW protocol a proof is structurally correct
if two properties are satisfied:

\begin{enumerate}[(a)]

\item The \emph{interlink} structure of all proof blocks is valid.

\item The first block of a proof for a chain $\chain$ is the first block
    $\chain$, the \emph{genesis} block.

\end{enumerate}

From property (b) of NIPoPoWs, we derive that the protocol is resilient to
premining because any chain that does not start from Bitcoin's $genesis$ block,
$\genesis$, results to a proof that also does not start from $\genesis$. Premined
chains start with blocks different than $\genesis$, hence proofs that describe
premined chains are invalid by definition.

\subsubsection{An attack} Previous implementation does require the existence of
underlying chain's $genesis$, exposing the verifier to premining attacks. Such
an attack can be launched by an adversary that mines new blocks on a chain
$\chain_p$ which is prior to the Bitcoin chain $\chain$ as displayed
in~\ref{figure:premining_attack_nipopow}.
\input{figures/premining_attack_nipopow.tex}

Proofs for events in $\chain_p$ cannot be contested by an honest party, since
chain $\chain_p$ includes more proof-of-work than $\chain$, and thus proofs
with higher score can be generated.

\subsubsection{Mitigation}

We can mitigate this vulnerability by initializing the smart contract with the
$genesis$ block of the underlying blockchain (in our case, Bitcoin) persisting
$genesis$ in storage. For every phase, we add an assertion that the first block
of the proof must be equal to $genesis$. The needed changes are shown in
Algorithms~\ref{algo:avoid_premining_ctor}, ~\ref{algo:avoid_premining_submit}
and~\ref{algo:avoid_premining_contest}.

These operation do not affect the cost of the verifier because the extra data
saved in storage is constant and small in size (32 bytes).

% \input{algorithms/avoid_premining.tex}

\section{Storage vs Memory}

We will first demonstrate the difference in gas usage between storage and
memory for a smart contract in Solidity. Suppose we have the following simple
contract:

\lstinputlisting[style=customc, captionpos=b, label={listing:storage_memory},
caption={Solidity test for storage and memory}]{code/StorageVsMemory.sol}
\todo{Highlight code}

Function \texttt{withStorage()} populates an array saved in storage and
function \texttt{withMemory()} populates an array saved in memory. We
initialize the sizes of the arrays by passing the variable \textsf{size} to the
contract constructor. We run this piece of code for \texttt{size} from 1 to
100. The results are displayed at Figure~\ref{figure:memory_vs_storage}. For
\textsf{size} equal to 100, the gas expended is 53,574 gas units using memory
and 2,569,848 using storage which is almost 50 times more expensive. This code
was compiled with Solidity version 0.6.6 with optimizations
enabled\footnote{This version of Solidity compiler, which was the latest at the
time this paper was published, did not optimize-out any of the variables.}. The
EVM we used  was Ganache at the latest Constantinople~\cite{constantinople}
fork. It is obvious that, if there is the option to use memory instead of
storage in the design of smart contracts, the choice of memory greatly benefits
the user.

\input{figures/memory_vs_storage.tex}

\section{The Hash-and-Resubmit Pattern}

We now introduce a novel design pattern for Solidity smart contracts that
results into significant gas optimization due to the elimination of expensive
storage operations. We first introduce our pattern, and display how smart
contracts benefit from using it. Then, we proceed into integrating our pattern
in the NIPoPoW protocol, and we analyze the performance in comparison with
previous work~\cite{gglou}.

\subsection{Motivation}
It is essential for smart contracts to store data in the blockchain. However,
interacting with the storage of a contract is among the most expensive
operations of the EVM~\cite{wood, buterin}. Therefore, only necessary data
should be stored and redundancy should be avoided when possible. This is
contrary to conventional software architecture, where storage is considered
cheap. Usually, the performance of data access in traditional systems is
related with time. In Ethereum, however, performance is related to gas
consumption. Access to persistent data costs a substantial amount of gas, which
has a direct monetary value.

One way to mitigate gas cost of reading variables
from the blockchain is to declare them as public. This leads to the creation of a
\emph{getter} function in the background, allowing free access to the value of
the variable. But this treatment does not prevent the initial population of
storage data and write operations which are significantly expensive for large
sizes of data.

By using the \emph{hash-and-resubmit} pattern, large structures are omitted
from storage entirely, and are contained in memory. When a function call is
performed, the signature and arguments of the function are included in the
transactions field of the body of a block. The contents of blocks are public to
the network, therefore this information is locally available to full nodes. By
simply observing blocks, a node retrieves data sent to the contract by other
users. To interact publicly with this data without the utilization storage, the
node \emph{resends} the observed data to the blockchain. The concept of
resending data is redundant in conventional systems. However, this technique
is very efficient to use in Solidity due to the significantly lower gas cost
of memory operations in relation with storage operations.

\subsection{Related patterns} Towards implementing gas-efficient smart
contracts~\cite{contract-opt-1, contract-opt-2, slither, madmax}, several
methodologies have been proposed. In order to eliminate storage operations
using data signatures, the utilization of IPFS~\cite{ipfs} is proposed by
\cite{ipfs-1} and~\cite{ipfs-2}. However, these solutions do not address
availability, which is one of our main requirements. Furthermore,
\cite{logs} uses logs to replace storage in a similar manner, sparing a great
amount of gas. However, this approach does not address consistency, which is
also one of our critical targets. Lastly, ~\cite{memory-array} proposes an
efficient manner to replace read storage operations, but does not address write
operations.

\subsection{Applicability}
We now list the requirements an application needs to meet in order to benefit
from the \emph{hash-and-resubmit} pattern:

\begin{enumerate}
    \item The application is a Solidity smart contract.
    \item Read/write operations are performed in large arrays that exist in
        storage. Using the pattern for variables of small size may result in
        negligible gain or even performance loss.
    \item A full node observes function calls to the smart contract.
\end{enumerate}

\subsection{Participants and collaborators} The first participant is the
smart contract $\contract$ that accepts function calls. Another participant is
the invoker $\invoker$, who dispatches a large array $\data_0$ to $\contract$
via a function \texttt{\proc$_1$}($\data_0$). Note that $\data_0$ is
potentially processed in $\proc_1$, resulting to $\data$. The last participant
is the observer $\observer$, who is a full node that observes transactions
towards $\contract$ in the blockchain. This is possible because nodes maintain
the blockchain locally. After observation, $\observer$ retrieves data $\data$.
Since this is an off-chain operation, a malicious $\observer$ potentially
alters $\data$ before interacting with $\contract$. We denote the
potentially modified $\data$ as $\datas$. Finally, $\observer$ acts as an
invoker by making a new call to $\contract$, \texttt{\proc$_2$}($\datas$). The
verification that $\data = \datas$, which is a prerequisite for the secure
functionality of the underlying contract, consists a part of the pattern and is
performed in \texttt{\proc$_2$}.

\subsection{Implementation} The implementation of this pattern is
divided in two parts. The first part covers how $\datas$ is retrieved by
$\observer$, whereas in the second part the verification of $\data=\datas$ is
realized. The challenge here is twofold:

\begin{enumerate}

    \item Availability: $\observer$ must be able to retrieve $\data$ without
        the need of accessing on-chain data.

    \item Consistency: $\observer$ must be prevented from dispatching $\datas$
        that differs from $\data$ which is a product of originally submitted
        $\data_0$.

\end{enumerate}

\emph{Hash-and-resubmit} technique is performed in two
stages to face these challenges: (a) the \emph{hash} phase, which addresses
\emph{consistency}, and (b) the \emph{resubmit} phase which addresses
\emph{availability} and \emph{consistency}.

\subsubsection{Addressing Availability} During the \emph{hash} phase,
$\invoker$ makes the function call \texttt{\proc}$_1$($\data_0$). This
transaction, which includes a function signature (\texttt{\proc$_1$}) and the
corresponding data ($\data_0$), is added in a block by a miner. Due to
blockchain's transparency, the observer of \texttt{\proc}$_1$, $\observer$,
retrieves a copy of $\data_0$ from the calldata, without the need of accessing contract data. In
turn, $\observer$ performs \emph{locally} the same set of on-chain instructions
operated on $\data_0$, generating $\data$. Thus, availability is addressed
through observability.

\subsubsection{Addressing Consistency} We prevent an adversary $\observer$
from dispatching data $\datas\neq\data$ by storing the \emph{signature} of
$\data$ in the contract's state during the execution of \texttt{\proc$_1$(.)} by
$\invoker$. In the context of Solidity, a signature of a structure is the
digest of the structure's \emph{hash}. The pre-compiled \texttt{sha256} is
convenient to use in Solidity, however we can make use of any cryptographic
hash function \textsf{H()}: \[\textsf{hash} \gets \textsf{H}(\textsf{d})\]
Then, in \emph{rehash} phase, the verification is performed by comparing the
stored digest of $\data$ with the digest of $\datas$:
\[\textsf{require}(\textsf{hash} = \texttt{H}(\datas))\] \noindent In Solidity,
the size of this digest is 32 bytes. To persist such a small value in the
contract's memory only adds a constant, negligible cost overhead. We illustrate
the application of the \emph{hash-and-resubmit} pattern in
Figure~\ref{fig:har-pattern}.

\begin{figure*}[h]
    \begin{center} \includegraphics[width=0.8\textwidth]{figures/har-pattern.pdf}
    \end{center}

    \caption{The \emph{hash-and-resubmit} pattern. First, an invoker calls
        \proc$_1$($\data_0$). $\data_0$ is processed \emph{on-chain} and
        $\data$ is generated. The signature of $\data$ is stored in the
        blockchain as the digest of a hash function \textsf{H}(.). Then,
        a full node that observes invocations of $\proc_1$ retrieves $\data_0$,
        and generates $\data$ by performing the analogous processing on
        $\data_0$ \emph{off-chain}. An adversarial observer dispatches
        $\datas$, where $\datas$$\neq$$\data$. Finally, the nodes invoke
        $\proc_2$(.). In $\proc_2$, the validation of input data is performed,
        reverting the function call if the signatures of the input does not
        match with the signature of the originally processed data. By applying
        a \emph{hash-and-resubmit pattern}, only the fixed-size signature of
        $\data$ is stored to the contract's state, replacing arbitrarily large
        structures.}

        \label{fig:har-pattern}
\end{figure*}


\subsection{Sample} We now demonstrate the usage of the
hash-and-resubmit pattern with a simplistic example. We create a smart contract
that orchestrates a game between two players, $\pla$ and $\plb$. The winner is
the player with the most valuable array. The interaction between players
through the smart contract is realized in two phases: (a) the submit phase and
(b) the contest phase.

\noindent\textbf{Submit phase:} $\pla$ submits an N-sized array, $\arra$, and
becomes the $\holder$ of the contract.

\noindent\textbf{Contest phase:} $\plb$ submits $\arrb$. If the result of
\textsf{compare}($\arrb$, $\arra$) is \true, then $\plb$ becomes the holder.

\input{algorithms/har-game}

We provide a simple implementation for \textsf{compare}, but we can consider
any notion of comparison, since the pattern is abstracted from such
implementation details.

We make use of the \emph{hash-and-resubmit} pattern by prompting $\plb$ to
provide \emph{two} arrays to the contract during contest phase: (a) $\arras$,
which is the originally submitted array by $\pla$, possibly modified by $\plb$,
and (b) $\arrb$, which is the contesting array.

We provide two implementations of the above described game.
In Algorithm~\ref{alg:game-storage} we display the storage implementation,
while in Algorithm~\ref{alg:game-memory} we show the implementation
embedding the \emph{hash-and-resubmit} pattern.


\subsection{Gas analysis} The gas consumption of the two above
implementations is displayed in Figure~\ref{fig:har-example}. By using the
\emph{hash-and-resubmit} pattern, the aggregated gas consumption for
\textsf{submit} and \textsf{contest} is decreased by 95\%. This significantly
affects the efficiency and applicability of the contract. Note that the storage
implementation exceeds the Ethereum block gas limit ($10{,}000{,}000$ gas as of
June 2020), for arrays of size 500 and above, contrary to the optimized
version, which consumes approximately only $1/10^{th}$ of the block gas limit
for arrays of $1{,}000$ elements.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.6 \columnwidth]{figures/har-example.pdf}
\end{center}
\caption{Gas-cost reduction using the \emph{hash-and-resubmit} pattern (lower
is better). By avoiding gas-heavy storage operations, the aggregated cost of
\textsf{submit} and \textsf{contest} is decreased by 95\%.}
\label{fig:har-example}
\end{figure}

\subsection{Consequences} The most obvious consequence of applying the
\emph{hash-and-resubmit} pattern is the circumvention of storage
structures, a benefit that saves a substantial amount of gas, especially when
these structures are large. To that extent, smart contracts that exceed the
Ethereum block gas limit and benefit sufficiently for the alleviation of
storage structures are becoming practical. Furthermore, the pattern enables
off-chain transactions, significantly improving the performance of smart
contracts.

\subsection{Known Uses} To our knowledge, we are the first to address
consistency and availability by combining blockchain's transparency with data
structures signatures in a manner that eliminates storage variables from
smart contracts.

\subsection{Enabling NIPoPoWs} We now present how the
\emph{hash-and-resubmit} pattern is used in the context of the NIPoPoW
superlight client. The NIPoPoW verifier adheres to a submit-and-contest schema
where the inputs of the functions are arrays that are processed on-chain, and a
node observes function calls towards the smart contract.  Therefore, it
makes a suitable case for our pattern.

In the \emph{submit} phase, a \emph{proof} is submitted. In the case of falsity, it
is contested by another user in \emph{contest} phase. The contester is a node
that monitors the traffic of the verifier. The input of \textsf{submit}
function includes the submit proof ($\pis$) that indicates the occurrence of an
\emph{event} ($e$) in the source chain, and the input of \textsf{contest}
function includes a contesting proof ($\pic$). A successful contest of $\pis$
is realized when $\pic$ has a better score~\cite{nipopows}. In this section, we
will not examine the score evaluation process since it is irrelevant to the
pattern. The size of proofs is dictated by the value $m$. We consider $m$ = 15
to be sufficiently secure~\cite{nipopows}.

In previous work, NIPoPoW proofs are maintained on-chain, resulting in
extensive storage operations that limit the applicability of the verifier
considerably. In our implementation, proofs are not stored on-chain, and $\pis$
is retrieved by the contester from the calldata. Since we
assume a trustless network, $\pis$ is altered by an adversarial contester. We denote
the potentially changed $\pis$ as $\pisa$. In \emph{contest} phase, $\pisa$ and
$\pic$ are dispatched in order to enable the \emph{hash-and-resubmit} pattern.

For our analysis, we create a blockchain similar to the Bitcoin chain with the
addition of the interlink structure in each block as in~\cite{gglou}. Our chain
spans 650,000 blocks, representing a slightly enhanced Bitcoin
chain\footnote{Bitcoin spans $633{,}022$ blocks as of June 2020. Metrics by
https://www.blockchain.com/}. From the tip of our chain, we branch two
sub-chains that span 100 and 200 additional blocks respectively, as illustrated
in Figure~\ref{fig:chains}. Then, we use the smaller chain to create $\pis$,
and the larger chain to create $\pic$. We apply the protocol by submitting
$\pis$, and contesting with $\pic$. The contest is successful, since $\pic$
represents a chain consisting of greater number of blocks than $\pis$,
therefore encapsulating more proof-of-work. We select this setting as it
provides maximum code coverage, and it describes the most gas-heavy scenario
for the verifier.

In Algorithm~\ref{alg:har-nipopow} we show how \emph{hash-and-resubmit} pattern
is embedded into the NIPoPoW client.

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=0.6\columnwidth]{figures/nipopow-subm-cont}
    \end{center}
    \caption{Forked chains for our gas analysis.}
    \label{fig:chains}
\end{figure}

In Figure~\ref{fig:har-nipopow}, we display how the \emph{hash-and-resubmit}
provides an improved implementation compared to previous work. The graph
illustrates the aggregated cost of \emph{submit} and \emph{contest} phases for
each implementation. We observe that, by using the \emph{hash-and-resubmit}
pattern, we achieve to increase the performance of the contract by 50\%. This
is a decisive step towards creating a practical superlight client.

Note that gas consumption generally follows an ascending trend, however it
is not a monotonically increasing function. This is due to the fact that
NIPoPoWs are probabilistic structures, the size of which is determined by the
distribution of superblocks within the underlying chain. A proof that is
constructed for a chain of a certain size can be larger than a proof
constructed for a slightly smaller chain, resulting in non-monotonic increase
of gas consumption between consecutive values of proof sizes.

\input{algorithms/har-nipopow}

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=0.6\columnwidth]{figures/har-nipopows.pdf}
    \end{center}
    \caption{Performance improvement using hash-and-resubmit pattern in
        NIPoPoWs compared to previous work for a secure value of $m$ (lower is
        better). The gas consumption is decreased by approximately 50\%.}
    \label{fig:har-nipopow}
\end{figure}

\section{Hash-and-Resubmit Variations}
In order to enable selective dispatch of a segment of interest, different
hashing schemas can be adopted, such as Merkle Trees ~\cite{merkle} and Merkle Mountain
Ranges ~\cite{mmr-1, mmr-2}. In this variation of the pattern, which we term
\emph{merkle-hash-and-resubmit}, the signature of an array $\data$ is
Merkle Tree Root (MTR). In the \emph{resubmit} phase, $\data[m{:}n]$ is
dispatched, accompanied by the siblings that reconstruct the MTR of
$\data$.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=1\linewidth]{figures/merkle-har.pdf}
    \end{center}
    \caption{\textbf{I.} The calculation of root in \emph{hash} phase.
    \textbf{II.} The verification of the root in \emph{resubmit} phase.
    \textsf{H}($k$) denotes the digest of element $k$. \textsf{H}($kl$) denotes the
    result of \textsf{H}(\textsf{H}($k$) $|$ \textsf{H}($l$))}
    \label{fig:merkle-har}
\end{figure}

This variation of the pattern removes the burden of sending redundant data,
however it implies on-chain construction and validation of the Merkle
construction. In order to construct a MTR for an array $\data$,
$|\data|$ hashes are needed for the leafs of the MT, and $|\data| -
1$ hashes are needed for the intermediate nodes. For the verification, the
segment of interest $\data[m{:}n]$ and the siblings of the MT are hashed.
The size of siblings is approximately $log_2(|\data|)$. The process of
constructing and verifying the MTR is displayed in Figure
~\ref{fig:merkle-har}.

In Solidity, different hashing operations vary in cost. An invocation of
\textsf{sha256}($\data$), copies $\data$ in memory, and then the
\emph{CALL} instruction is performed by the EVM that calls a pre-compiled
contract. At the current state of the EVM, \emph{CALL} costs 700 gas units, and
the gas paid for every word when expanding memory is 3 gas units~\cite{wood}.
Consequently, the expression $1 \times \textsf{sha256}(\data)$ costs less than
$|\data| \times $\textsf{sha256}(1) operations. A different cost policy applies
for \textsf{keccak}~\cite{keccak} hash function, where hashing costs 30 gas
units plus 6 additional gas far each word for input data~\cite{wood}. The usage
of \textsf{keccak} dramatically increases the performance in comparison with
\textsf{sha256}, and performs better than plain rehashing if the product of
on-chain processing is sufficiently larger than the originally dispatched data.
Costs of all related operations are listed in Table~\ref{tab:operations-gas}.

\input{tables/operations-gas}

The merkle variation can be potentially improved by dividing $\data$ in
chunks larger than 1 element. We leave this analysis for future work.

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=0.5\columnwidth]{figures/har-vs-mhar.pdf}
    \end{center}
    \caption{Trade-offs between \emph{hash-and-resubmit} variations. In the
    vertical axis the gas consumption is displayed. In the horizontal axis the
    size of $\data$. The size of $d_0$ is 10KB bytes, and the hash functions we
    use are the pre-compiled \texttt{sha256} and \texttt{keccak}.}
    \label{fig:har-vs-mhar}
\end{figure}


In Table~\ref{tab:har-vs-mhar} we display the operations needed for hashing and
verifying the underlying data for both variations of the pattern as a function
of data size. In Figure~\ref{fig:har-vs-mhar} we demonstrate the gas
consumption for dispatched data of 10KB, and varying size of on-chain
process product.

\input{tables/har-vs-mhar}

\section{Removing Look-up Structures}

Now that we freely eliminate large arrays, we can focus on other
types of storage variables. The challenge we face is that the protocol of
NIPoPoWs depends on a Directed Acyclic Graph (DAG) of blocks which is a
mutable hashmap. This DAG is needed because interlinks of superblocks can be
adversarially defined. By using DAG, the set of ancestor blocks of a block is
extracted by performing a simple graph search. For the evaluation of the
predicate, the set of \emph{ancestors} of the best blockchain tip is used.
Ancestors are created to avoid an adversary who presents an honest chain but
skips the blocks of interest.

This logic is intuitive and efficient to implement in most traditional
programming languages such as C++, JAVA, Python, JavaScript, etc. However, as
our analysis demonstrates, such an implementation in Solidity is significantly
expensive. Albeit Solidity supports constant-time look-up structures, hashmaps
are only contained in storage. This affects the performance of the client,
especially for large proofs.

We make a keen observation regarding potential positions of the \emph{block of
interest} in proofs, which leads us to the construction of an architecture that
does not require a DAG, the ancestors or other complementary structures. To
support this claim, we adopt the notation from~\cite{nipopows}. We also
consider the predicate $\pred$ to be of the type: ``does block $\boi$ exist
inside proof $\pr$?'', where $\boi$ denotes the block of interest of proof
$\pr$. The entity that performs the submission is $\es$, and the entity that
initiates a contest is $\ec$.

\subsection{Position of Block of Interest} NIPoPoWs are sets of sampled
interlinked blocks, meaning that they can be perceived as chains. Since proofs
$\pis$ and $\pic$ differ, a fork is created at the index of
their last common ancestor ($\lca$). The block of interest lies at a certain
index within $\pis$ and indicates a stable predicate~\cite{nipopows,
generic-client} that is $\true$ for $\pis$. A submission in which $\boi$ is
absent from $\pis$ is aimless, because it automatically fails since no element
of $\pis$ satisfies $\pred$. On the contrary, $\pic$ tries to prove the
\emph{falseness} of the underlying predicate. This means that, if the block of
interest is included in $\pic$, then the contest is aimless. We freely use the
term aimless to also characterize components that are included in such actions
i.e.\ aimless proof, aimless blocks etc. We use the term meaningful to describe
non-aimless actions and components.

\newcommand{\block}{\mathsf{B}}

In the NIPoPoW protocol, proofs' segments $\pis\{{{:}}\lca\}$ and
$\pic\{{:}\lca\}$ are merged to prevent adversaries from skipping
blocks, and the predicate is evaluated against $\pis\{{:}\lca\} \cup
\pic\{{:}\lca\}$. We observe that $\pic\{{:}\lca\}$ can be omitted, because no
block $\block$ exists such that \{$\block : \block \notin \pis\{{:}\lca\} \land
\block \in \pic\{{:}\lca\}$\} where $\block$ results into positive evaluation
of the predicate. This is due to the fact that, in a meaningful contest, $\boi$
is not included in $\pic$. Consequently, $\pic$ is only meaningful if it forks
$\pis$ at a block that is prior to $\boi$.

\renewcommand{\block}{}

In Figure~\ref{fig:boi-position} we display a fork of two proofs. Solid lines
connect blocks of $\pis$ and dashed lines connect blocks of $\pic$. By
examining which scenarios are meaningful based on different positions of the
block of interest, we observe that blocks \texttt{B}, \texttt{C} and \texttt{E}
do not qualify, because they are included in $\pic$. Block \texttt{A} is
included in $\pis\{{:}\lca\}$, which means that $\pic$ is an aimless contest
because the $\lca$ comes after the block of interest. Therefore, A is an
aimless block as a component of an aimless contest. Given this configuration,
the only meaningful block of interest is \texttt{D} and its predecessors (which
we leave out from this figure).

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=0.6\columnwidth]{figures/boi-position.pdf}
    \end{center}
    \caption{Fork of two proofs.
    Solid lines connect blocks of $\pis$,
    and dashed lines connect blocks of $\pic$.
    In this configuration,
    blocks in dashed circles are aimless blocks of interest, and the block
    in the solid circle is a meaningful block of interest. Blocks B, C and E are
    aimless because they exist in $\pic$. Block A is aimless because it
    belongs to the subchain $\pis\{{:}\lca\}$
    }
    \label{fig:boi-position}
\end{figure}


\subsection{Minimal Fork} By combining the above observations, we
derive that $\pic$ can be truncated into $\pic\{\lca{:}\}$ without affecting
the correctness of the protocol. We term this truncated proof $\pitr$.
Security is preserved by requiring $\pitr$ to be a \emph{minimal fork} of
$\pis$. A minimal fork is a fork chain that shares exactly one common block
with the main chain. A proof $\tilde\pi$, which is minimal fork of $\pi$, has
the following attributes:

\begin{enumerate}
\item $\pi\{lca\} = \tilde\pi[0]$
\item $\pi\{lca{:}\} \cap \tilde\pi[1{:}] = \emptyset$
\end{enumerate}

By requiring $\pitr$ to be a minimal fork of $\pis$, we prevent adversaries
from dispatching an augmented $\pitr$ to claim better score against $\pis$.
Such an attempt is displayed in Figure~\ref{fig:adversary-minimal-fork}.

\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.5\columnwidth]{figures/adversary-minimal-fork.pdf}
    \end{center}
    \caption{An adversary attests to contest with a malformed proof. Adversary
        proof consists of blocks \{A, X, B, C, D'\} that achieve better score
        against submit proof \{A, B, C, D\}. This attempt is rejected due to
        the minimal-fork requirement.}
    \label{fig:adversary-minimal-fork}
\end{figure}

In Algorithm~\ref{alg:minimal-fork}, we show how the minimal fork technique is
incorporated into our client replacing DAG and ancestors. In
Figure~\ref{fig:minimal-fork} we show how the performance of the client
improves. We use the same test case as in \emph{hash-and-resubmit}.

\input{algorithms/minimal-fork}

By applying the minimal-fork technique, he achieve a 55\% decrease in gas
consumption. \emph{Submit} phase now costs {4{,}700{,}000} gas, and
the \emph{contest} phase costs {4{,}900{,}000} million gas. This is a notable
result, since each phase now fits inside an Ethereum block.


\begin{figure}[h]
    \begin{center}
        \includegraphics[width=0.6\columnwidth]{figures/minimal-fork.pdf}
    \end{center}
    \caption{Performance improvement using minimal fork (lower is better). The
        gas consumption is decreased by approximately 55\%.}
    \label{fig:minimal-fork}
\end{figure}

\section{Processing Fewer Blocks}

The complexity of most demanding on-chain operations of the verifier are linear
to the size of the proof. This includes the proof validation and the evaluation
of score. We now present two techniques that allow for equivalent operations of
constant complexity.

\subsection{Optimistic Schemes} In smart contracts, in order to ensure that users
comply with the underlying application's rules, certain actions need to be
performed on-chain, e.g. verification of data, balance checks, etc. In a
different approach, actions that deviate from the protocol are reverted after
honest users indicate them, not allowing diverging entities to gain advantages.
Such applications that do not check the validity of actions by default, but
rather depend on the intervention of honest users are characterized
``optimistic''. In the Ethereum community, several projects ~\cite{piza,
plasma, rollups-1, rollups-2} have emerged that incorporate the notion of
optimistic interactions. We observe that such a schema can be embedded into the
NIPoPoW protocol, resulting in significant performance gain.

We discussed how the verification in the NIPoPoW protocol is realized in two
phases. In \emph{submit} phase, the verification of the $\pis$ is performed.
This is necessary in order to prevent adversaries from injecting blocks that do
not belong to the chain, or changing existing blocks. A proof is valid for
submission if it is \emph{structurally correct}. Correctly structured NIPoPoWs
have the following requirements: (a) the first block of the proof is the
genesis block of the underlying blockchain and (b) every block has a valid
interlink.

Asserting the existence of genesis in the first index of a proof is an
inexpensive operation of constant complexity. However, confirming the interlink
correctness of all blocks is a process of linear complexity to the size of the
proof. Albeit the verification is performed in memory, sufficiently large
proofs result into costly submissions since their validation consist the most
demanding function of the \emph{submit} phase. In
Table~\ref{tab:valid-interlink-cost} we display the cost of
\textsf{valid-interlink} function which determines the structural correctness
of a proof in comparison with the overall gas used in \textsf{submit}.

\input{tables/valid-interlink-cost}

\newcommand{\dispute}{\emph{dispute\ }}

\subsection{Dispute Phase} We observe that the addition of a phase in
our protocol alleviates the burden of verifying all elements of the proof by
enabling the indication of an individual incorrect block. This phase, which we
term \dispute phase, leverages selective verification of the submitted proof at
a certain index. As a constant operation, this significantly reduces the gas
cost of the verification process.

In the NIPoPoW protocol, when a proof $\pis$ is submitted by $\es$, it is
retrieved by a node $\ec$ from the calldata and the proof is checked for its
validity \emph{off-chain}. We observe that, in
order to prove a structurally invalid $\pis$, $\ec$ only needs to
indicate the index in which $\pis$ fails the interlink verification. In the
protocol that incorporates \emph{dispute} phase, $\ec$ calls
$\textsf{dispute}$($\pisa$, $i$) for a structurally incorrect proof, where $i$
indicates the disputing index of $\pisa$. Therefore, only one block is
interpreted \emph{on-chain} rather than the entire span of $\pisa$.

Note that this additional phase does not imply increased rounds of
interactions between $\es$ and $\ec$. If $\pis$ is invalidated in
\emph{dispute} phase, then \emph{contest} phase is skipped. Similarly, if
$\pis$ is structurally correct, but represents a dishonest chain, then $\ec$
proceeds directly to \emph{contest} phase without the invocation of \emph{dispute}.

\input{tables/dispute-cost}

In Table~\ref{tab:dispute-cost} we display the gas consumption for
two independent cycles of interactions:
\begin{enumerate}
    \item \emph{Submit} and \emph{dispute} for is structurally incorrect
        $\pis$.
    \item \emph{Submit} and \emph{contest} for structurally correct
        $\pis$ that represents a dishonest chain.
\end{enumerate}

\noindent In Algorithm~\ref{alg:dispute-best-level}, we show the implementation
of the \emph{dispute} phase. The integration of \emph{dispute} phase leaves
\textsf{contest} unchanged.

\subsection{Isolating the Best Level} As we discussed, \emph{dispute}
and \emph{contest} phases are mutually exclusive. Unfortunately, the same
constant-time verification as in the \emph{dispute} phase cannot be applied in a
contest without increasing the rounds of interactions for the users. However,
we derive a major optimization for the \emph{contest} phase by observing the
process of score evaluation.

In NIPoPoWs, after the last common ancestor is found, each proof fork
is evaluated in terms of proof-of-work score. Each level encapsulates a different
score of proof-of-work, and the level with the best score is representative
of the underlying proof. Since the common blocks of the two proofs naturally
gather the same score, only the disjoint portions need to be addressed.
Consequently, the position of the $\lca$ determines the span of the proofs that
will be included in the score evaluation process. Furthermore, it is impossible
to determine the score of a proof in the \emph{submit} phase because the position
of $\lca$ is yet unknown.

After $\pis$ is retrieved from the calldata, the score of both proofs is
calculated. This means that the level in which each proof encapsulates the
most proof-of-work for each proof is known to $\ec$. In the light of this
observation, $\ec$ only submits the blocks which consist the \emph{best level}
of $\pic$. The number of these blocks is constant, as it is determined by the
security parameter $m$, which is irrelevant to the size of the underlying
blockchain. We illustrate the blocks that participate in the formulation of a
proof's score and the best level of contesting proof in
Figure~\ref{fig:score-at-levels}.

\begin{figure}[!h]
    \begin{center}
    \includegraphics[width=0.6\columnwidth]{figures/blocks-of-best-level.pdf}
    \end{center}
    \caption{Fork of two proofs. Striped blocks determine the
    score of each proof. Black blocks belong to the level that
    has the best score. Only black blocks are part of the best level of the
    contesting proof.}
    \label{fig:score-at-levels}
\end{figure}


The calculation of the best level of $\pic$ is an \emph{off-chain} process.
An adversarial $\ec$ is certainly able to dispatch a level of $\pic$ which is
different than the best level. However, this is an irrational action, since
different levels only undermine the score of $\pic$. On the contrary, due to
the consistency property of \emph{hash-and-resubmit}, $\pis$ cannot be altered.
We denote the best level of $\pitr$ as $\pitrl$.

In Algorithm~\ref{alg:dispute-best-level}, we show the implementation of the
\emph{contest} phase under the best-level enhancement. The utilization of
this methodology greatly increases the performance of the client,
because the complexity of the majority of $\textsf{contest}$ functions is
related to the size of $\pic$. In Table~\ref{tab:best-level-cost}, we
demonstrate the difference in gas consumption in the \emph{contest} phase after
using \emph{best-level}. The performance of most functions is increased by
approximately 85\%. This is due to the fact that the size of $\pic$ is
decreased accordingly. For $m=15$, $\pitrl$ consists of 31 blocks, while
$\pitr$ consists of 200 blocks.  Notably, the calculation of score for $\pitrl$
needs 97\% less gas. We achieve such a discrepancy because the process of score
calculation for multiple levels demands the use of a temporary hashmap which is
a storage structure. In contrast, the evaluation of the score of an individual level
is performed entirely in memory.

\input{tables/best-level-cost}

In Figure ~\ref{fig:dispute-best-level}, we illustrate the performance gain of
the client using \emph{dispute} phase and the best-level contesting proof. The
aggregated gas consumption of \emph{submit} and \emph{contest} phases is
reduced to 3,500,000 gas. This is a critical threshold regarding applicability of
the contract, since a cycle of interactions now effortlessly fits inside a
single Ethereum block.

\input{algorithms/dispute-best-level}

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=0.6\columnwidth]{figures/dispute-best-level.pdf}
    \end{center}
    \caption{Performance improvement using optimistic schema in submit phase
        and best level proof in contesting proof (lower is better). Gas
        consumption is decreased by approximately 65\%.}
    \label{fig:dispute-best-level}
\end{figure}
\newpage



% -------------------------------------------------------------------------
%                               Old text
% -------------------------------------------------------------------------

% \subsection{Refining Ancestors}
%
% We observe that in the code of the verifier the existence of
% \texttt{ancestors}$_s$ structure is redundant. The lifetime of the structure is
% limited within the last lines of each phase, and serves the role of temporary
% storing the result of \texttt{evaluatePredicate}, and then it is deleted. The
% same functionality can be achieved if the \texttt{ancestor}$_s$ is saved in
% memory rather than in the storage of the contract. The changes needed in order
% to convert \texttt{ancestors}$_s$ into a memory structure is displayed in
% Algorithm~\ref{algorithm:ancestors_in_memory}. Here, \texttt{ancestors}$_s$ is
% replaced with \texttt{ancestors}$_{m}$, which is saved in memory. Since the
% lifetime of memory structures is restricted within the function call, there is
% no need for deletion.
%
% \input{algorithms/ancestors_to_memory.tex}
%
% The elimination of storage structures results to better performance of the
% contract. In Table~\ref{table:old_gas_usage_no_ans} we show how such a subtle
% refinement can lead to a sizable performance gain.
%
% \input{tables/old_gas_usage_no_anc.tex}
%
% \subsection{Refining Predicate}
%
% \todo{predicate is not needed in contest phase}
%
% \subsection{Hash and resubmit}
%
% In previous work, storage was needed to persist submitted proofs and perform
% contests. In this subsection we show our novel approach to securely verify
% proofs without the utilization of persistent storage. We term this technique
% \emph{hash-and-resubmit}.
%
% The rationale is to demand from $E_{cont}$ to provide two proofs to the
% contract during contest phase: (a) $\pi_{exist}$, which is a copy of the
% originally submitted proof $\pi_{orig}$, and (b) $\pi_{cont}$, which is the
% contesting proof. The challenge here, is twofold:
%
% \begin{enumerate}
%
%     \item Availability: $E_{cont}$ must be able to retrieve a valid copy of $\pi_{orig}$,
%         $\pi_{exist}$ without the need of accessing on-chain data.
%
%     \item Reliability: We must prevent $E_{cont}$ from dispatching
%         $\tilde\pi_{exist}$ which differs from $\pi_{orig}$.
%
% \end{enumerate}
%
% As its name suggests, \emph{hash-and-resubmit} technique is performed in two
% phases to face this challenge: the \emph{hash} phase which addresses
% \emph{reliability}, and the \emph{resubmit} phase which addresses
% \emph{availability}.
%
% % The corresponding changes are displayed in
% % Algorithms~\ref{algorithm:submit_hash_and_resubmit}
%
% \paragraph{Addressing Availability:}
%
% During submit-phase, $E_{subm}$ makes a call to the function
% \texttt{submitEventProof}. This transaction, which consists of (a) the function
% call and (b) the corresponding data, is added to a block by a miner. As a
% blockchain transaction, the data of the function call are public to the
% network. Due to blockchain's transparency, $E_{cont}$ can retrieve
% $\pi_{orig}$ without the need of accessing contract data. This data space is
% called \emph{calldata}, and we make use of it to fetch $\pi_{exist}$, which is
% an exact copy of $\pi_{orig}$, originally submitted by $E_{subm}$. Note that
% this process is done off-chain. $E_{cont}$ does not have to interact with the
% client.
%
% \paragraph{Addressing Reliability:}
%
% We prevent an adversary from altering $\pi_{exist}$ by storing the hash of
% $\pi_{orig}$ in contract's state during submit phase and then verifying that
% $\pi_{exist}$ has the same hash. The operation of hashing the proof and storing
% the digest is cheap as shown in figure~\ref{figure:hash_proof_gas}. We
% calculate the digest by invoking the pre-compiled \texttt{sha256} hash function
% of Solidity:
%
% \[\texttt{digest = sha256(abi.encodePacked(proof))}\] The size of the digest of
% a hash is 32 bytes. To persist such a small value in contract's memory only
% adds a constant, negligible cost overhead to our implementation. This low-cost
% technique allows us to provide vastly improved performance compared to the
% previous work as we will show in the following subsections.
%
% \input{algorithms/hash_and_resubmit.tex}
%
% \input{figures/hash_proof_gas.tex}
%
% \subsection{Removing DAG and ancestors}
%
% In this subsection we show how these two structures can be discarded from the
% client entirely. As shown in table~\ref{table:old_gas_usage}, the most
% demanding operation is the creation and population of DAG and ancestors. We
% show how ancestors structure can be improved, but the performance gain from
% this change still fails to deliver a client that is practical in a real
% setting.
%
% Now that we have achieved to freely retrieve $\pi_{exist}$, we can start
% sketching methodologies that benefit from this schema but another challenge we
% had to face is that the protocol of NIPoPoWs dependents on DAG which is a
% hashmap data structure. While such data structures are very efficient and
% useful data structures, in Solidity, hashmaps can only be contained in storage,
% which is unpromising for an efficient client design. Having that in mind, we
% created a schema that does not require the DAG, ancestors or any other
% complementary structures.
%
%
% \subsubsection{Using subset}
%
% Our first realization was that instead of creating a DAG of $\pi_{exist}$ and
% $\pi_{cont}$, we can rather require
%
% \[ \pi_{exist}\{:\textrm{LCA}\} \subseteq \pi_{cont}\{:\textrm{LCA}\} \]
% This way, we avoid the burden of maintaining auxiliary structures DAG and
% ancestors on-chain. The implementation of \texttt{subset} is displayed in
% listing~\ref{listing:subset}. The complexity of the function is \[
% \mathcal{O}(|\pi_{exist}\{:LCA\}| + |\pi_{cont}\{:LCA\}]|) \]
%
% \lstinputlisting[style=customc, captionpos=b, label={listing:subset},
% caption={Implementation of subset}]{code/Subset.sol}
%
% \input{figures/subset_usage.tex}
%
% The gas consumption difference between $subset$ and $DAG+ancestors$ is
% displayed at figure~\ref{figure:DAG_vs_subset}. $Subset$ solution is
% approximately 2.7 times more efficient.
%
% \input{figures/DAG_vs_subset.tex}
%
% \subsubsection{Subset complexity and limitations}
%
% Requiring $\pi_{exist}$ to be a subset of $\pi_{cont}$ greatly reduces gas, but
% the complexity of the $subset$ algorithm is high since both proofs have to be
% iterated from $Gen$ until $lca_e$ and $lca_c$, respectively. Generally, we
% expect from an adversary to provide a proof of a chain that is a fork of the
% honest chain at some point relatively close to the tip. This is due to the fact
% that the ability of an adversary to sustain a fork chain exponentially weakens
% as the honest chain progresses. This means that the length of $\pi$, $|\pi|$ is
% expected to be considerably close to $|\pi[:lca]|$, and thus the complexity of
% \texttt{subset()} effectively becomes $\mathcal{O}(2|\pi|)$.
%
% In realistic cases, where LCA lies around index 250 of the proof, the gas cost
% of \texttt{subset()} is approximately 20,000,000 gas units, which makes it
% inapplicable for real blockchains since it exceeds the block gas limit of the
% Ethereum blockchain by far.
%
% \subsubsection{Position of block of interest}
%
% By analyzing the trade-offs of $subset$, we concluded that there is a more
% efficient way to treat storage elimination. In general, the concept of $subset$
% facilitated the case in which the block of interest belongs in the sub-proof
% $\pi_{exist}[:LCA]$. But in this case, both $\pi_{exist}$ and $\pi_{cont}$
% contain the block of interest at some index, as can be seen in
% figure~\ref{figure:after_subset}. Consequently, $\pi_{cont}$ cannot contradict
% the existence of the block of interest and the predicate is evaluated $true$
% for both proofs. This means that if (a) $\pi_{exist}$ is structurally correct
% and (b) the block of interest is in $\pi_{exist}[:lca_{e}]$, then we can safely
% conclude that contesting with $\pi_{cont}$ is redundant. Therefore,
% $E_{cont}$ can simply send $\pi_{cont}$[lca:] to the verifier. The
% truncation of $\pi_{cont}$ to $\pi_{cont}[lca_{c}:]$ can be easily addressed
% by $E_{cont}$, since $\pi_{exist}$ is accessible from the contract's calldata
% and both proofs can be iterated off-chain.
%
% \subsubsection{Disjoint proofs}
%
% We will refer to the truncated contesting proof as $\pi_{cont}^{tr}$ and to
% $lca_{e}$ simply as $lca$. For the aforementioned, the following statements are
% true:
%
% \begin{enumerate}[(a)]
%     \item  $\pi_{exist}[0]$ = $genesis$
%     \item  $\pi_{exist}[lca]$ = $\pi_{cont}^{tr}[0]$
% \end{enumerate}
%
% The requirement that needs to be satisfied is
% \[\pi_{exist}\{lca+1:\} \cap \pi_{cont}^{tr}\{1:\} = \emptyset \]
%
% The implementation of this operation is shown in
% listing~\ref{listing:disjoint}.
%
% \lstinputlisting[style=customc, captionpos=b, label={listing:disjoint},
% caption={Implementation for disjoint proofs}]{code/Disjoint.sol}
%
% The complexity of \texttt{disjoint()} is
% \[ \mathcal{O}(|\pi_{exist}[lca_{e}:]| \times
% |\pi_{cont}^{tr}|) \]
%
% % \begin{itemize}
% %
% %     \item
% %         This is not vulnerable to DOS attacks
% %
% % \end{itemize}
